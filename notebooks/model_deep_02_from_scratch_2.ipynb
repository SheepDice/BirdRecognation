{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dropout, GlobalAveragePooling2D, Dense, LeakyReLU, Conv2D, MaxPooling2D,\\\n",
    "      Flatten, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, Callback\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras import Model\n",
    "from timeit import default_timer as timer\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import const\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diverted_images_train_path = os.path.join(const.DATASET_CLEAN, \"train\")\n",
    "# diverted_images_valid_path = os.path.join(const.DATASET_CLEAN, \"valid\")\n",
    "# diverted_images_test_path = os.path.join(const.DATASET_CLEAN, \"test\")\n",
    "\n",
    "# Remplacez 'chemin_vers_le_nouveau_repertoire' par le chemin du nouveau répertoire\n",
    "const.DATASET = \"..\\\\data\\\\dataset_reduit\"\n",
    "\n",
    "diverted_images_train_path = os.path.join(const.DATASET, \"train\")\n",
    "diverted_images_valid_path = os.path.join(const.DATASET, \"valid\")\n",
    "diverted_images_test_path = os.path.join(const.DATASET, \"test\")\n",
    "\n",
    "\n",
    "for path in [diverted_images_train_path, diverted_images_valid_path, diverted_images_test_path]:\n",
    "    if not os.path.isdir(path):\n",
    "        print(f\"Le dossier {path} n'existe pas. Veuillez vérifier le chemin.\")\n",
    "\n",
    "class TimingCallback(Callback):\n",
    "    def __init__(self, logs={}):\n",
    "        self.logs=[]\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.starttime = timer()\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(timer()-self.starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 980 images belonging to 10 classes.\n",
      "Found 210 images belonging to 10 classes.\n",
      "Found 210 images belonging to 10 classes.\n",
      "Epoch 1/20\n",
      "61/61 [==============================] - 70s 1s/step - loss: 2.3916 - acc: 0.2562 - mean_absolute_error: 0.1585 - val_loss: 3.6448 - val_acc: 0.1010 - val_mean_absolute_error: 0.1775 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "61/61 [==============================] - 70s 1s/step - loss: 1.9951 - acc: 0.3485 - mean_absolute_error: 0.1461 - val_loss: 4.2898 - val_acc: 0.1394 - val_mean_absolute_error: 0.1721 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.0624 - acc: 0.3185 - mean_absolute_error: 0.1473\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "61/61 [==============================] - 72s 1s/step - loss: 2.0624 - acc: 0.3185 - mean_absolute_error: 0.1473 - val_loss: 3.7046 - val_acc: 0.1490 - val_mean_absolute_error: 0.1732 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "61/61 [==============================] - 72s 1s/step - loss: 1.8397 - acc: 0.3817 - mean_absolute_error: 0.1373 - val_loss: 3.3379 - val_acc: 0.1250 - val_mean_absolute_error: 0.1745 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "61/61 [==============================] - 71s 1s/step - loss: 1.8114 - acc: 0.3724 - mean_absolute_error: 0.1385 - val_loss: 4.2653 - val_acc: 0.1875 - val_mean_absolute_error: 0.1701 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "61/61 [==============================] - 73s 1s/step - loss: 1.7305 - acc: 0.4129 - mean_absolute_error: 0.1356 - val_loss: 5.7653 - val_acc: 0.1635 - val_mean_absolute_error: 0.1710 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "61/61 [==============================] - 73s 1s/step - loss: 1.5569 - acc: 0.4450 - mean_absolute_error: 0.1288 - val_loss: 3.5557 - val_acc: 0.2260 - val_mean_absolute_error: 0.1626 - lr: 1.0000e-04\n",
      "Epoch 7: early stopping\n",
      "14/14 [==============================] - 2s 125ms/step - loss: 3.1753 - acc: 0.2762 - mean_absolute_error: 0.1555\n",
      "[3.175328493118286, 0.27619048953056335, 0.15547506511211395]\n",
      "0.22596152126789093\n"
     ]
    }
   ],
   "source": [
    "#0.64-0.62\n",
    "# meilleur modèle avec augmentation des données\n",
    "batch_size = 16\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(diverted_images_train_path, target_size=(224, 224), \n",
    "                                                    batch_size=batch_size, class_mode='categorical')\n",
    "valid_generator = valid_datagen.flow_from_directory(diverted_images_valid_path, target_size=(224, 224), \n",
    "                                                    batch_size=batch_size, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(diverted_images_test_path, target_size=(224, 224), \n",
    "                                                  batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor=\"val_loss\", patience=2, min_delta=0.01, factor=0.1, cooldown=4, verbose=1)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01, verbose=1, mode='min', monitor='val_loss')\n",
    "time_callback = TimingCallback()\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=20,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "61/61 [==============================] - 71s 1s/step - loss: 2.6808 - acc: 0.1992 - mean_absolute_error: 0.1653 - val_loss: 2.8446 - val_acc: 0.1394 - val_mean_absolute_error: 0.1777 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "61/61 [==============================] - 71s 1s/step - loss: 2.2090 - acc: 0.3143 - mean_absolute_error: 0.1473 - val_loss: 4.3691 - val_acc: 0.1058 - val_mean_absolute_error: 0.1755 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.0728 - acc: 0.3299 - mean_absolute_error: 0.1445\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "61/61 [==============================] - 73s 1s/step - loss: 2.0728 - acc: 0.3299 - mean_absolute_error: 0.1445 - val_loss: 5.4859 - val_acc: 0.1058 - val_mean_absolute_error: 0.1783 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "61/61 [==============================] - 74s 1s/step - loss: 1.7814 - acc: 0.4077 - mean_absolute_error: 0.1335 - val_loss: 4.8326 - val_acc: 0.1154 - val_mean_absolute_error: 0.1760 - lr: 1.0000e-04\n",
      "Epoch 4: early stopping\n",
      "14/14 [==============================] - 1s 81ms/step - loss: 4.8011 - acc: 0.1429 - mean_absolute_error: 0.1740\n",
      "[4.801071643829346, 0.1428571492433548, 0.1740269809961319]\n",
      "0.11538460850715637\n"
     ]
    }
   ],
   "source": [
    "#0.56-0.60\n",
    "#modèle sequentiel avec Batchnormalization\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=20,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "61/61 [==============================] - 71s 1s/step - loss: 2.4231 - acc: 0.2490 - mean_absolute_error: 0.1611 - val_loss: 2.9219 - val_acc: 0.1587 - val_mean_absolute_error: 0.1778 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "61/61 [==============================] - 70s 1s/step - loss: 1.8163 - acc: 0.4015 - mean_absolute_error: 0.1378 - val_loss: 3.5625 - val_acc: 0.1106 - val_mean_absolute_error: 0.1763 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6009 - acc: 0.4388 - mean_absolute_error: 0.1296\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "61/61 [==============================] - 71s 1s/step - loss: 1.6009 - acc: 0.4388 - mean_absolute_error: 0.1296 - val_loss: 6.0837 - val_acc: 0.1442 - val_mean_absolute_error: 0.1725 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "61/61 [==============================] - 75s 1s/step - loss: 1.9147 - acc: 0.3745 - mean_absolute_error: 0.1404 - val_loss: 5.1418 - val_acc: 0.1250 - val_mean_absolute_error: 0.1757 - lr: 1.0000e-04\n",
      "Epoch 4: early stopping\n",
      "14/14 [==============================] - 1s 97ms/step - loss: 5.1572 - acc: 0.1048 - mean_absolute_error: 0.1767\n",
      "[5.157219409942627, 0.10476190596818924, 0.17672035098075867]\n",
      "0.1249999925494194\n"
     ]
    }
   ],
   "source": [
    "#0.59-0.56\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=20,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 980 images belonging to 10 classes.\n",
      "Found 210 images belonging to 10 classes.\n",
      "Found 210 images belonging to 10 classes.\n",
      "Epoch 1/20\n",
      "61/61 [==============================] - 135s 2s/step - loss: 2.7395 - acc: 0.2033 - mean_absolute_error: 0.1658 - val_loss: 6.0258 - val_acc: 0.1923 - val_mean_absolute_error: 0.1624 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "61/61 [==============================] - 156s 3s/step - loss: 2.4348 - acc: 0.2676 - mean_absolute_error: 0.1556 - val_loss: 4.3190 - val_acc: 0.1442 - val_mean_absolute_error: 0.1723 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "61/61 [==============================] - 148s 2s/step - loss: 2.0776 - acc: 0.3278 - mean_absolute_error: 0.1454 - val_loss: 6.0378 - val_acc: 0.1058 - val_mean_absolute_error: 0.1781 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "61/61 [==============================] - 144s 2s/step - loss: 2.1367 - acc: 0.3133 - mean_absolute_error: 0.1448 - val_loss: 4.2688 - val_acc: 0.1490 - val_mean_absolute_error: 0.1725 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "61/61 [==============================] - 148s 2s/step - loss: 1.9340 - acc: 0.3693 - mean_absolute_error: 0.1378 - val_loss: 4.1175 - val_acc: 0.1442 - val_mean_absolute_error: 0.1704 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "61/61 [==============================] - 145s 2s/step - loss: 1.7775 - acc: 0.4129 - mean_absolute_error: 0.1315 - val_loss: 3.4541 - val_acc: 0.1875 - val_mean_absolute_error: 0.1606 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "61/61 [==============================] - 436s 7s/step - loss: 1.6530 - acc: 0.4303 - mean_absolute_error: 0.1269 - val_loss: 3.4400 - val_acc: 0.2067 - val_mean_absolute_error: 0.1580 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "61/61 [==============================] - 135s 2s/step - loss: 1.5549 - acc: 0.4772 - mean_absolute_error: 0.1210 - val_loss: 2.6677 - val_acc: 0.2308 - val_mean_absolute_error: 0.1594 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "61/61 [==============================] - 147s 2s/step - loss: 1.4928 - acc: 0.4969 - mean_absolute_error: 0.1160 - val_loss: 2.6678 - val_acc: 0.2452 - val_mean_absolute_error: 0.1607 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4015 - acc: 0.5311 - mean_absolute_error: 0.1139\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "61/61 [==============================] - 147s 2s/step - loss: 1.4015 - acc: 0.5311 - mean_absolute_error: 0.1139 - val_loss: 5.3363 - val_acc: 0.3029 - val_mean_absolute_error: 0.1473 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "61/61 [==============================] - 147s 2s/step - loss: 1.2941 - acc: 0.5654 - mean_absolute_error: 0.1098 - val_loss: 1.8460 - val_acc: 0.4423 - val_mean_absolute_error: 0.1286 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "61/61 [==============================] - 155s 3s/step - loss: 1.2872 - acc: 0.5508 - mean_absolute_error: 0.1092 - val_loss: 1.6192 - val_acc: 0.5240 - val_mean_absolute_error: 0.1219 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "61/61 [==============================] - 154s 3s/step - loss: 1.3045 - acc: 0.5612 - mean_absolute_error: 0.1084 - val_loss: 1.5373 - val_acc: 0.5481 - val_mean_absolute_error: 0.1181 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "61/61 [==============================] - 144s 2s/step - loss: 1.2510 - acc: 0.5705 - mean_absolute_error: 0.1059 - val_loss: 0.9084 - val_acc: 0.6923 - val_mean_absolute_error: 0.0891 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "61/61 [==============================] - 150s 2s/step - loss: 1.1945 - acc: 0.5922 - mean_absolute_error: 0.1028 - val_loss: 1.2770 - val_acc: 0.6106 - val_mean_absolute_error: 0.1047 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1616 - acc: 0.5965 - mean_absolute_error: 0.1007\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "61/61 [==============================] - 148s 2s/step - loss: 1.1616 - acc: 0.5965 - mean_absolute_error: 0.1007 - val_loss: 1.2743 - val_acc: 0.6106 - val_mean_absolute_error: 0.1045 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "61/61 [==============================] - 145s 2s/step - loss: 1.2181 - acc: 0.5975 - mean_absolute_error: 0.1008 - val_loss: 1.0436 - val_acc: 0.6635 - val_mean_absolute_error: 0.0947 - lr: 1.0000e-05\n",
      "Epoch 17: early stopping\n",
      "14/14 [==============================] - 5s 355ms/step - loss: 0.9495 - acc: 0.6810 - mean_absolute_error: 0.0920\n",
      "[0.9494931101799011, 0.6809524297714233, 0.09197670221328735]\n",
      "0.6634615063667297\n"
     ]
    }
   ],
   "source": [
    "#augmentation de la complexité du modèle, avec augmentation de données\n",
    "batch_size = 16\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(diverted_images_train_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "valid_generator = valid_datagen.flow_from_directory(diverted_images_valid_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(diverted_images_test_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor=\"val_loss\", patience=2, min_delta=0.01, factor=0.1, cooldown=4, verbose=1)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01, verbose=1, mode='min', monitor='val_loss')\n",
    "time_callback = TimingCallback()\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=20,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 980 images belonging to 10 classes.\n",
      "Found 210 images belonging to 10 classes.\n",
      "Found 210 images belonging to 10 classes.\n",
      "Epoch 1/20\n",
      "61/61 [==============================] - 160s 3s/step - loss: 3.0241 - acc: 0.2106 - mean_absolute_error: 0.1641 - val_loss: 13.9857 - val_acc: 0.1010 - val_mean_absolute_error: 0.1797 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "61/61 [==============================] - 155s 3s/step - loss: 2.9061 - acc: 0.2573 - mean_absolute_error: 0.1554 - val_loss: 13.0044 - val_acc: 0.1010 - val_mean_absolute_error: 0.1798 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "61/61 [==============================] - 158s 3s/step - loss: 2.5673 - acc: 0.2936 - mean_absolute_error: 0.1484 - val_loss: 4.7631 - val_acc: 0.1106 - val_mean_absolute_error: 0.1780 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "61/61 [==============================] - 147s 2s/step - loss: 2.2306 - acc: 0.3568 - mean_absolute_error: 0.1401 - val_loss: 9.0159 - val_acc: 0.1010 - val_mean_absolute_error: 0.1802 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.0470 - acc: 0.3921 - mean_absolute_error: 0.1347\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "61/61 [==============================] - 148s 2s/step - loss: 2.0470 - acc: 0.3921 - mean_absolute_error: 0.1347 - val_loss: 6.9245 - val_acc: 0.1202 - val_mean_absolute_error: 0.1747 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "61/61 [==============================] - 147s 2s/step - loss: 1.7367 - acc: 0.4398 - mean_absolute_error: 0.1244 - val_loss: 5.5962 - val_acc: 0.1106 - val_mean_absolute_error: 0.1767 - lr: 1.0000e-04\n",
      "Epoch 6: early stopping\n",
      "14/14 [==============================] - 5s 322ms/step - loss: 5.4110 - acc: 0.1143 - mean_absolute_error: 0.1765\n",
      "[5.411023139953613, 0.11428572237491608, 0.1764831840991974]\n",
      "0.11057691276073456\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(diverted_images_train_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "valid_generator = valid_datagen.flow_from_directory(diverted_images_valid_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(diverted_images_test_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor=\"val_loss\", patience=2, min_delta=0.01, factor=0.1, cooldown=4, verbose=1)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01, verbose=1, mode='min', monitor='val_loss')\n",
    "time_callback = TimingCallback()\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))  \n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.35))  \n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.45))  \n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.55)) \n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.6))  \n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=20,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 980 images belonging to 10 classes.\n",
      "Found 210 images belonging to 10 classes.\n",
      "Found 210 images belonging to 10 classes.\n",
      "Epoch 1/20\n",
      "61/61 [==============================] - 2s 35ms/step - loss: 2.9491 - acc: 0.2085 - mean_absolute_error: 0.1649 - val_loss: 2.3824 - val_acc: 0.2692 - val_mean_absolute_error: 0.1567 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "61/61 [==============================] - 2s 33ms/step - loss: 2.0964 - acc: 0.2562 - mean_absolute_error: 0.1615 - val_loss: 2.0288 - val_acc: 0.2788 - val_mean_absolute_error: 0.1569 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "61/61 [==============================] - 2s 33ms/step - loss: 1.9463 - acc: 0.2842 - mean_absolute_error: 0.1580 - val_loss: 1.9205 - val_acc: 0.2837 - val_mean_absolute_error: 0.1526 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "61/61 [==============================] - 2s 33ms/step - loss: 1.8967 - acc: 0.2905 - mean_absolute_error: 0.1554 - val_loss: 1.8990 - val_acc: 0.3125 - val_mean_absolute_error: 0.1496 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "61/61 [==============================] - 2s 36ms/step - loss: 1.8776 - acc: 0.3008 - mean_absolute_error: 0.1542 - val_loss: 1.9063 - val_acc: 0.3221 - val_mean_absolute_error: 0.1516 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "61/61 [==============================] - 3s 42ms/step - loss: 1.8507 - acc: 0.3122 - mean_absolute_error: 0.1537 - val_loss: 1.8422 - val_acc: 0.3558 - val_mean_absolute_error: 0.1500 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "61/61 [==============================] - 2s 40ms/step - loss: 1.8353 - acc: 0.3133 - mean_absolute_error: 0.1526 - val_loss: 1.9297 - val_acc: 0.3750 - val_mean_absolute_error: 0.1514 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.8331 - acc: 0.3112 - mean_absolute_error: 0.1534\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "61/61 [==============================] - 3s 41ms/step - loss: 1.8331 - acc: 0.3112 - mean_absolute_error: 0.1534 - val_loss: 1.8967 - val_acc: 0.3413 - val_mean_absolute_error: 0.1483 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "61/61 [==============================] - 3s 41ms/step - loss: 1.7573 - acc: 0.3610 - mean_absolute_error: 0.1496 - val_loss: 1.8159 - val_acc: 0.3606 - val_mean_absolute_error: 0.1482 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "61/61 [==============================] - 3s 42ms/step - loss: 1.7445 - acc: 0.3444 - mean_absolute_error: 0.1502 - val_loss: 1.8282 - val_acc: 0.3558 - val_mean_absolute_error: 0.1475 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "61/61 [==============================] - 3s 41ms/step - loss: 1.7477 - acc: 0.3465 - mean_absolute_error: 0.1496 - val_loss: 1.8229 - val_acc: 0.3558 - val_mean_absolute_error: 0.1475 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "61/61 [==============================] - 2s 40ms/step - loss: 1.7259 - acc: 0.3506 - mean_absolute_error: 0.1485 - val_loss: 1.8259 - val_acc: 0.3558 - val_mean_absolute_error: 0.1475 - lr: 1.0000e-04\n",
      "Epoch 12: early stopping\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.6382 - acc: 0.3619 - mean_absolute_error: 0.1448\n",
      "[1.6381874084472656, 0.3619047701358795, 0.14479166269302368]\n",
      "0.35576921701431274\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor=\"val_loss\", patience=2, min_delta=0.01, factor=0.1, cooldown=4, verbose=1)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01, verbose=1, mode='min', monitor='val_loss')\n",
    "time_callback = TimingCallback()\n",
    "\n",
    "train_generator = ImageDataGenerator().flow_from_directory(diverted_images_train_path, target_size=(224, 224), batch_size=batch_size)\n",
    "valid_generator = ImageDataGenerator().flow_from_directory(diverted_images_valid_path, target_size=(224, 224), batch_size=batch_size)\n",
    "test_generator = ImageDataGenerator().flow_from_directory(diverted_images_test_path, target_size=(224, 224), batch_size=batch_size)\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "def build_model():\n",
    "    input_layer = Input(shape=(224, 224, 3))\n",
    "    x = GlobalAveragePooling2D()(input_layer)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=input_layer, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "#model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=20,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 980 images belonging to 10 classes.\n",
      "Found 210 images belonging to 10 classes.\n",
      "Found 210 images belonging to 10 classes.\n",
      "Nombre de classes dans les données d'entraînement : 10\n",
      "Nombre de classes dans les données de validation : 10\n",
      "Nombre de classes dans les données de test : 10\n",
      "Epoch 1/50\n",
      "61/61 [==============================] - 62s 1s/step - loss: 3.5952 - acc: 0.2386 - mean_absolute_error: 0.1569 - val_loss: 3.1557 - val_acc: 0.1010 - val_mean_absolute_error: 0.1782 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - 62s 1s/step - loss: 2.7649 - acc: 0.3724 - mean_absolute_error: 0.1360 - val_loss: 3.2807 - val_acc: 0.1010 - val_mean_absolute_error: 0.1768 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.6476 - acc: 0.3869 - mean_absolute_error: 0.1325\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "61/61 [==============================] - 65s 1s/step - loss: 2.6476 - acc: 0.3869 - mean_absolute_error: 0.1325 - val_loss: 3.6885 - val_acc: 0.1058 - val_mean_absolute_error: 0.1767 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - 76s 1s/step - loss: 2.4053 - acc: 0.4512 - mean_absolute_error: 0.1217 - val_loss: 3.5594 - val_acc: 0.1106 - val_mean_absolute_error: 0.1735 - lr: 1.0000e-05\n",
      "Epoch 4: early stopping\n",
      "14/14 [==============================] - 3s 248ms/step - loss: 3.5380 - acc: 0.1429 - mean_absolute_error: 0.1716\n",
      "[3.537951707839966, 0.1428571492433548, 0.17160218954086304]\n",
      "0.11057691276073456\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, Callback\n",
    "from tensorflow.keras import regularizers\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "class TimingCallback(Callback):\n",
    "    def __init__(self, logs={}):\n",
    "        self.logs=[]\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.starttime = timer()\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(timer()-self.starttime)\n",
    "\n",
    "# Chemins vers les dossiers de données\n",
    "const.DATASET = \"..\\\\data\\\\dataset_reduit\"\n",
    "diverted_images_train_path = os.path.join(const.DATASET, \"train\")\n",
    "diverted_images_valid_path = os.path.join(const.DATASET, \"valid\")\n",
    "diverted_images_test_path = os.path.join(const.DATASET, \"test\")\n",
    "\n",
    "# Vérifiez que les dossiers existent\n",
    "for path in [diverted_images_train_path, diverted_images_valid_path, diverted_images_test_path]:\n",
    "    if not os.path.isdir(path):\n",
    "        print(f\"Le dossier {path} n'existe pas. Veuillez vérifier le chemin.\")\n",
    "\n",
    "# Paramètres du modèle\n",
    "batch_size = 16\n",
    "\n",
    "# Préparation des générateurs de données\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Création des générateurs de données\n",
    "train_generator = train_datagen.flow_from_directory(diverted_images_train_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "valid_generator = valid_datagen.flow_from_directory(diverted_images_valid_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(diverted_images_test_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "# Affichage du nombre de classes\n",
    "print(f\"Nombre de classes dans les données d'entraînement : {train_generator.num_classes}\")\n",
    "print(f\"Nombre de classes dans les données de validation : {valid_generator.num_classes}\")\n",
    "print(f\"Nombre de classes dans les données de test : {test_generator.num_classes}\")\n",
    "\n",
    "# Construction du modèle\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=(224, 224, 3), kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Ajoutez plus de couches ici si nécessaire\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Compilation du modèle\n",
    "model = build_model()\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "# Entraînement du modèle\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=50, \n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "# Évaluation du modèle\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 980 images belonging to 10 classes.\n",
      "Found 210 images belonging to 10 classes.\n",
      "Found 210 images belonging to 10 classes.\n",
      "Epoch 1/20\n",
      "61/61 [==============================] - 137s 2s/step - loss: 2.7480 - acc: 0.1992 - mean_absolute_error: 0.1651 - val_loss: 4.3200 - val_acc: 0.1010 - val_mean_absolute_error: 0.1799 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "61/61 [==============================] - 177s 3s/step - loss: 1.9863 - acc: 0.3548 - mean_absolute_error: 0.1422 - val_loss: 5.4800 - val_acc: 0.0962 - val_mean_absolute_error: 0.1806 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.8057 - acc: 0.4066 - mean_absolute_error: 0.1330\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "61/61 [==============================] - 170s 3s/step - loss: 1.8057 - acc: 0.4066 - mean_absolute_error: 0.1330 - val_loss: 4.3526 - val_acc: 0.0962 - val_mean_absolute_error: 0.1799 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "61/61 [==============================] - 170s 3s/step - loss: 1.6312 - acc: 0.4730 - mean_absolute_error: 0.1231 - val_loss: 4.3066 - val_acc: 0.1106 - val_mean_absolute_error: 0.1785 - lr: 1.0000e-05\n",
      "Epoch 5/20\n",
      "61/61 [==============================] - 178s 3s/step - loss: 1.6541 - acc: 0.4554 - mean_absolute_error: 0.1240 - val_loss: 4.3881 - val_acc: 0.1058 - val_mean_absolute_error: 0.1761 - lr: 1.0000e-05\n",
      "Epoch 6/20\n",
      " 1/61 [..............................] - ETA: 3:23 - loss: 1.0074 - acc: 0.6250 - mean_absolute_error: 0.0943"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 70\u001b[0m\n\u001b[0;32m     67\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_absolute_error\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 70\u001b[0m history_diverted \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mtrain_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mvalid_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mreduce_learning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_generator)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_accuracy)\n",
      "File \u001b[1;32mc:\\Users\\yonie\\Documents\\GitHub\\reco_oiseau_jan24bds\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\yonie\\Documents\\GitHub\\reco_oiseau_jan24bds\\venv\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\yonie\\Documents\\GitHub\\reco_oiseau_jan24bds\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\yonie\\Documents\\GitHub\\reco_oiseau_jan24bds\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\yonie\\Documents\\GitHub\\reco_oiseau_jan24bds\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\yonie\\Documents\\GitHub\\reco_oiseau_jan24bds\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yonie\\Documents\\GitHub\\reco_oiseau_jan24bds\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\yonie\\Documents\\GitHub\\reco_oiseau_jan24bds\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\yonie\\Documents\\GitHub\\reco_oiseau_jan24bds\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#augmentation de la complexité du modèle, avec augmentation de données\n",
    "batch_size = 16\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(diverted_images_train_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "valid_generator = valid_datagen.flow_from_directory(diverted_images_valid_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(diverted_images_test_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor=\"val_loss\", patience=2, min_delta=0.01, factor=0.1, cooldown=4, verbose=1)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01, verbose=1, mode='min', monitor='val_loss')\n",
    "time_callback = TimingCallback()\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=20,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "61/61 [==============================] - 184s 3s/step - loss: 2.5278 - acc: 0.2479 - mean_absolute_error: 0.1584 - val_loss: 7.9708 - val_acc: 0.1010 - val_mean_absolute_error: 0.1794 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "61/61 [==============================] - 191s 3s/step - loss: 2.3338 - acc: 0.2873 - mean_absolute_error: 0.1486 - val_loss: 52.5603 - val_acc: 0.1010 - val_mean_absolute_error: 0.1798 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.2728 - acc: 0.3122 - mean_absolute_error: 0.1477\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "61/61 [==============================] - 172s 3s/step - loss: 2.2728 - acc: 0.3122 - mean_absolute_error: 0.1477 - val_loss: 18.3670 - val_acc: 0.1010 - val_mean_absolute_error: 0.1796 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "61/61 [==============================] - 177s 3s/step - loss: 2.1578 - acc: 0.3392 - mean_absolute_error: 0.1439 - val_loss: 7.7942 - val_acc: 0.1154 - val_mean_absolute_error: 0.1775 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "61/61 [==============================] - 168s 3s/step - loss: 1.9813 - acc: 0.3600 - mean_absolute_error: 0.1389 - val_loss: 6.3658 - val_acc: 0.1106 - val_mean_absolute_error: 0.1773 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "61/61 [==============================] - 182s 3s/step - loss: 1.8151 - acc: 0.4087 - mean_absolute_error: 0.1342 - val_loss: 5.1902 - val_acc: 0.1298 - val_mean_absolute_error: 0.1734 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "61/61 [==============================] - 179s 3s/step - loss: 1.8183 - acc: 0.4046 - mean_absolute_error: 0.1322 - val_loss: 4.8079 - val_acc: 0.1442 - val_mean_absolute_error: 0.1710 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "61/61 [==============================] - 191s 3s/step - loss: 1.7851 - acc: 0.4212 - mean_absolute_error: 0.1300 - val_loss: 3.7668 - val_acc: 0.1635 - val_mean_absolute_error: 0.1666 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "61/61 [==============================] - 187s 3s/step - loss: 1.7773 - acc: 0.4346 - mean_absolute_error: 0.1280 - val_loss: 3.1127 - val_acc: 0.2500 - val_mean_absolute_error: 0.1551 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "61/61 [==============================] - 194s 3s/step - loss: 1.6525 - acc: 0.4564 - mean_absolute_error: 0.1259 - val_loss: 1.9657 - val_acc: 0.3846 - val_mean_absolute_error: 0.1351 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "61/61 [==============================] - 186s 3s/step - loss: 1.6703 - acc: 0.4523 - mean_absolute_error: 0.1264 - val_loss: 1.8133 - val_acc: 0.4423 - val_mean_absolute_error: 0.1321 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "61/61 [==============================] - 192s 3s/step - loss: 1.6035 - acc: 0.4803 - mean_absolute_error: 0.1210 - val_loss: 1.4966 - val_acc: 0.5144 - val_mean_absolute_error: 0.1219 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "61/61 [==============================] - 194s 3s/step - loss: 1.5727 - acc: 0.4606 - mean_absolute_error: 0.1207 - val_loss: 1.4443 - val_acc: 0.5240 - val_mean_absolute_error: 0.1189 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "61/61 [==============================] - 222s 4s/step - loss: 1.5511 - acc: 0.4876 - mean_absolute_error: 0.1187 - val_loss: 1.7531 - val_acc: 0.4423 - val_mean_absolute_error: 0.1255 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5032 - acc: 0.5145 - mean_absolute_error: 0.1158\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "61/61 [==============================] - 205s 3s/step - loss: 1.5032 - acc: 0.5145 - mean_absolute_error: 0.1158 - val_loss: 1.6047 - val_acc: 0.4712 - val_mean_absolute_error: 0.1212 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "61/61 [==============================] - 176s 3s/step - loss: 1.4721 - acc: 0.5031 - mean_absolute_error: 0.1150 - val_loss: 1.3514 - val_acc: 0.5481 - val_mean_absolute_error: 0.1120 - lr: 1.0000e-05\n",
      "Epoch 17/20\n",
      "61/61 [==============================] - 166s 3s/step - loss: 1.4778 - acc: 0.4990 - mean_absolute_error: 0.1171 - val_loss: 1.3162 - val_acc: 0.5769 - val_mean_absolute_error: 0.1106 - lr: 1.0000e-05\n",
      "Epoch 18/20\n",
      "61/61 [==============================] - 157s 3s/step - loss: 1.4464 - acc: 0.5332 - mean_absolute_error: 0.1137 - val_loss: 1.2478 - val_acc: 0.6010 - val_mean_absolute_error: 0.1075 - lr: 1.0000e-05\n",
      "Epoch 19/20\n",
      "61/61 [==============================] - 152s 2s/step - loss: 1.4651 - acc: 0.5062 - mean_absolute_error: 0.1151 - val_loss: 1.2256 - val_acc: 0.6010 - val_mean_absolute_error: 0.1058 - lr: 1.0000e-05\n",
      "Epoch 20/20\n",
      "61/61 [==============================] - 165s 3s/step - loss: 1.4458 - acc: 0.5124 - mean_absolute_error: 0.1157 - val_loss: 1.2580 - val_acc: 0.6058 - val_mean_absolute_error: 0.1067 - lr: 1.0000e-05\n",
      "14/14 [==============================] - 5s 376ms/step - loss: 1.2242 - acc: 0.6286 - mean_absolute_error: 0.1036\n",
      "[1.224180817604065, 0.6285714507102966, 0.10362288355827332]\n",
      "0.6057692170143127\n"
     ]
    }
   ],
   "source": [
    "#conversion en fonctionnel du meilleur modèle obtenu plus haut\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_functional_model():\n",
    "    inputs = Input(shape=(224, 224, 3))\n",
    "\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "    x = Conv2D(512, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(512, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_functional_model()\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=20,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 980 images belonging to 10 classes.\n",
      "Found 210 images belonging to 10 classes.\n",
      "Found 210 images belonging to 10 classes.\n",
      "Epoch 1/40\n",
      "61/61 [==============================] - 137s 2s/step - loss: 2.8400 - acc: 0.2075 - mean_absolute_error: 0.1680 - val_loss: 7.6253 - val_acc: 0.1010 - val_mean_absolute_error: 0.1792 - lr: 0.0010\n",
      "Epoch 2/40\n",
      "61/61 [==============================] - 162s 3s/step - loss: 2.4322 - acc: 0.2313 - mean_absolute_error: 0.1609 - val_loss: 21.5498 - val_acc: 0.1010 - val_mean_absolute_error: 0.1798 - lr: 0.0010\n",
      "Epoch 3/40\n",
      "61/61 [==============================] - 154s 3s/step - loss: 2.2978 - acc: 0.2790 - mean_absolute_error: 0.1547 - val_loss: 7.2761 - val_acc: 0.1010 - val_mean_absolute_error: 0.1797 - lr: 0.0010\n",
      "Epoch 4/40\n",
      "61/61 [==============================] - 161s 3s/step - loss: 2.2806 - acc: 0.3019 - mean_absolute_error: 0.1515 - val_loss: 10.0905 - val_acc: 0.1058 - val_mean_absolute_error: 0.1769 - lr: 0.0010\n",
      "Epoch 5/40\n",
      "61/61 [==============================] - 158s 3s/step - loss: 2.1897 - acc: 0.3143 - mean_absolute_error: 0.1486 - val_loss: 5.6413 - val_acc: 0.1250 - val_mean_absolute_error: 0.1727 - lr: 0.0010\n",
      "Epoch 6/40\n",
      "61/61 [==============================] - 150s 2s/step - loss: 2.0739 - acc: 0.3226 - mean_absolute_error: 0.1487 - val_loss: 8.1425 - val_acc: 0.1010 - val_mean_absolute_error: 0.1796 - lr: 0.0010\n",
      "Epoch 7/40\n",
      "61/61 [==============================] - 161s 3s/step - loss: 1.9534 - acc: 0.3496 - mean_absolute_error: 0.1426 - val_loss: 4.9731 - val_acc: 0.1490 - val_mean_absolute_error: 0.1716 - lr: 0.0010\n",
      "Epoch 8/40\n",
      "61/61 [==============================] - 141s 2s/step - loss: 1.8621 - acc: 0.3631 - mean_absolute_error: 0.1413 - val_loss: 3.1991 - val_acc: 0.2452 - val_mean_absolute_error: 0.1604 - lr: 0.0010\n",
      "Epoch 9/40\n",
      "61/61 [==============================] - 146s 2s/step - loss: 1.7976 - acc: 0.3890 - mean_absolute_error: 0.1391 - val_loss: 2.6665 - val_acc: 0.2452 - val_mean_absolute_error: 0.1595 - lr: 0.0010\n",
      "Epoch 10/40\n",
      "61/61 [==============================] - 153s 3s/step - loss: 1.7397 - acc: 0.3952 - mean_absolute_error: 0.1377 - val_loss: 2.1913 - val_acc: 0.2452 - val_mean_absolute_error: 0.1518 - lr: 0.0010\n",
      "Epoch 11/40\n",
      "61/61 [==============================] - 154s 3s/step - loss: 1.6259 - acc: 0.4232 - mean_absolute_error: 0.1317 - val_loss: 2.2800 - val_acc: 0.3413 - val_mean_absolute_error: 0.1486 - lr: 0.0010\n",
      "Epoch 12/40\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5418 - acc: 0.4720 - mean_absolute_error: 0.1273\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "61/61 [==============================] - 150s 2s/step - loss: 1.5418 - acc: 0.4720 - mean_absolute_error: 0.1273 - val_loss: 12.3551 - val_acc: 0.1827 - val_mean_absolute_error: 0.1624 - lr: 0.0010\n",
      "Epoch 13/40\n",
      "61/61 [==============================] - 140s 2s/step - loss: 1.5910 - acc: 0.4512 - mean_absolute_error: 0.1289 - val_loss: 1.7253 - val_acc: 0.3990 - val_mean_absolute_error: 0.1346 - lr: 1.0000e-04\n",
      "Epoch 14/40\n",
      "61/61 [==============================] - 146s 2s/step - loss: 1.5076 - acc: 0.4761 - mean_absolute_error: 0.1259 - val_loss: 1.4904 - val_acc: 0.4760 - val_mean_absolute_error: 0.1287 - lr: 1.0000e-04\n",
      "Epoch 15/40\n",
      "61/61 [==============================] - 150s 2s/step - loss: 1.4977 - acc: 0.4761 - mean_absolute_error: 0.1260 - val_loss: 1.6722 - val_acc: 0.4231 - val_mean_absolute_error: 0.1326 - lr: 1.0000e-04\n",
      "Epoch 16/40\n",
      "61/61 [==============================] - 142s 2s/step - loss: 1.4534 - acc: 0.4896 - mean_absolute_error: 0.1239 - val_loss: 1.7534 - val_acc: 0.4279 - val_mean_absolute_error: 0.1336 - lr: 1.0000e-04\n",
      "Epoch 17/40\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.3920 - acc: 0.4886 - mean_absolute_error: 0.1212\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "61/61 [==============================] - 149s 2s/step - loss: 1.3920 - acc: 0.4886 - mean_absolute_error: 0.1212 - val_loss: 1.6460 - val_acc: 0.4615 - val_mean_absolute_error: 0.1295 - lr: 1.0000e-04\n",
      "Epoch 17: early stopping\n",
      "14/14 [==============================] - 5s 367ms/step - loss: 1.6474 - acc: 0.4619 - mean_absolute_error: 0.1308\n",
      "[1.6474121809005737, 0.4619047939777374, 0.13078100979328156]\n",
      "0.4615384340286255\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    diverted_images_train_path, \n",
    "    target_size=(224, 224), \n",
    "    batch_size=batch_size, \n",
    "    class_mode='categorical')\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    diverted_images_valid_path, \n",
    "    target_size=(224, 224), \n",
    "    batch_size=batch_size, \n",
    "    class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    diverted_images_test_path, \n",
    "    target_size=(224, 224), \n",
    "    batch_size=batch_size, \n",
    "    class_mode='categorical')\n",
    "\n",
    "\n",
    "def build_functional_model():\n",
    "    inputs = Input(shape=(224, 224, 3))\n",
    "\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "    x = Conv2D(512, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(512, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_functional_model()\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=40,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "61/61 [==============================] - 138s 2s/step - loss: 2.7765 - acc: 0.1878 - mean_absolute_error: 0.1654 - val_loss: 9.5268 - val_acc: 0.1058 - val_mean_absolute_error: 0.1782 - lr: 0.0010\n",
      "Epoch 2/40\n",
      "61/61 [==============================] - 155s 3s/step - loss: 2.6790 - acc: 0.2127 - mean_absolute_error: 0.1627 - val_loss: 7.0646 - val_acc: 0.0865 - val_mean_absolute_error: 0.1808 - lr: 0.0010\n",
      "Epoch 3/40\n",
      "61/61 [==============================] - 149s 2s/step - loss: 2.4837 - acc: 0.2479 - mean_absolute_error: 0.1564 - val_loss: 2.9813 - val_acc: 0.1779 - val_mean_absolute_error: 0.1638 - lr: 0.0010\n",
      "Epoch 4/40\n",
      "61/61 [==============================] - 145s 2s/step - loss: 2.2237 - acc: 0.2790 - mean_absolute_error: 0.1516 - val_loss: 6.9302 - val_acc: 0.1010 - val_mean_absolute_error: 0.1771 - lr: 0.0010\n",
      "Epoch 5/40\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.9755 - acc: 0.3537 - mean_absolute_error: 0.1415\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "61/61 [==============================] - 137s 2s/step - loss: 1.9755 - acc: 0.3537 - mean_absolute_error: 0.1415 - val_loss: 5.4969 - val_acc: 0.1058 - val_mean_absolute_error: 0.1765 - lr: 0.0010\n",
      "Epoch 6/40\n",
      "61/61 [==============================] - 146s 2s/step - loss: 1.8317 - acc: 0.3859 - mean_absolute_error: 0.1380 - val_loss: 2.4453 - val_acc: 0.2692 - val_mean_absolute_error: 0.1547 - lr: 1.0000e-04\n",
      "Epoch 7/40\n",
      "61/61 [==============================] - 146s 2s/step - loss: 1.7413 - acc: 0.4253 - mean_absolute_error: 0.1342 - val_loss: 2.3271 - val_acc: 0.2596 - val_mean_absolute_error: 0.1510 - lr: 1.0000e-04\n",
      "Epoch 8/40\n",
      "61/61 [==============================] - 134s 2s/step - loss: 1.7544 - acc: 0.4149 - mean_absolute_error: 0.1337 - val_loss: 2.0574 - val_acc: 0.3317 - val_mean_absolute_error: 0.1451 - lr: 1.0000e-04\n",
      "Epoch 9/40\n",
      "61/61 [==============================] - 144s 2s/step - loss: 1.7425 - acc: 0.4087 - mean_absolute_error: 0.1339 - val_loss: 2.0646 - val_acc: 0.3606 - val_mean_absolute_error: 0.1429 - lr: 1.0000e-04\n",
      "Epoch 10/40\n",
      "61/61 [==============================] - 144s 2s/step - loss: 1.6987 - acc: 0.4388 - mean_absolute_error: 0.1318 - val_loss: 1.7629 - val_acc: 0.4183 - val_mean_absolute_error: 0.1348 - lr: 1.0000e-04\n",
      "Epoch 11/40\n",
      "61/61 [==============================] - 136s 2s/step - loss: 1.6394 - acc: 0.4426 - mean_absolute_error: 0.1277 - val_loss: 1.4497 - val_acc: 0.4952 - val_mean_absolute_error: 0.1244 - lr: 1.0000e-04\n",
      "Epoch 12/40\n",
      "61/61 [==============================] - 145s 2s/step - loss: 1.6042 - acc: 0.4627 - mean_absolute_error: 0.1251 - val_loss: 1.2216 - val_acc: 0.5865 - val_mean_absolute_error: 0.1149 - lr: 1.0000e-04\n",
      "Epoch 13/40\n",
      "61/61 [==============================] - 145s 2s/step - loss: 1.6098 - acc: 0.4678 - mean_absolute_error: 0.1255 - val_loss: 1.0619 - val_acc: 0.6442 - val_mean_absolute_error: 0.1053 - lr: 1.0000e-04\n",
      "Epoch 14/40\n",
      "61/61 [==============================] - 135s 2s/step - loss: 1.5438 - acc: 0.4927 - mean_absolute_error: 0.1224 - val_loss: 1.1085 - val_acc: 0.5913 - val_mean_absolute_error: 0.1063 - lr: 1.0000e-04\n",
      "Epoch 15/40\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5981 - acc: 0.4647 - mean_absolute_error: 0.1249\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "61/61 [==============================] - 143s 2s/step - loss: 1.5981 - acc: 0.4647 - mean_absolute_error: 0.1249 - val_loss: 1.1735 - val_acc: 0.5529 - val_mean_absolute_error: 0.1112 - lr: 1.0000e-04\n",
      "Epoch 16/40\n",
      "61/61 [==============================] - 145s 2s/step - loss: 1.5382 - acc: 0.4782 - mean_absolute_error: 0.1215 - val_loss: 1.0908 - val_acc: 0.5962 - val_mean_absolute_error: 0.1064 - lr: 1.0000e-05\n",
      "Epoch 16: early stopping\n",
      "14/14 [==============================] - 5s 370ms/step - loss: 1.1387 - acc: 0.5952 - mean_absolute_error: 0.1067\n",
      "[1.1386513710021973, 0.5952380895614624, 0.10673677176237106]\n",
      "0.5961537957191467\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_functional_model():\n",
    "    inputs = Input(shape=(224, 224, 3))\n",
    "\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "    x = Conv2D(512, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(512, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_functional_model()\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=40,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
