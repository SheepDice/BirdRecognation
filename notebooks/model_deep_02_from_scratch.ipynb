{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dropout, GlobalAveragePooling2D, Dense, LeakyReLU, Conv2D, MaxPooling2D,\\\n",
    "      Flatten, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, Callback\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras import Model\n",
    "from timeit import default_timer as timer\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import const\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "diverted_images_train_path = os.path.join(const.DATASET_REDUIT, \"train\")\n",
    "diverted_images_valid_path = os.path.join(const.DATASET_REDUIT, \"valid\")\n",
    "diverted_images_test_path = os.path.join(const.DATASET_REDUIT, \"test\")\n",
    "\n",
    "class TimingCallback(Callback):\n",
    "    def __init__(self, logs={}):\n",
    "        self.logs=[]\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.starttime = timer()\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(timer()-self.starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2829 images belonging to 23 classes.\n",
      "Found 575 images belonging to 23 classes.\n",
      "Found 575 images belonging to 23 classes.\n",
      "Epoch 1/20\n",
      "176/176 [==============================] - 76s 428ms/step - loss: 4.6349 - acc: 0.0885 - mean_absolute_error: 0.0819 - val_loss: 3.0508 - val_acc: 0.0696 - val_mean_absolute_error: 0.0823 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "176/176 [==============================] - 4s 22ms/step - loss: 3.0296 - acc: 0.0860 - mean_absolute_error: 0.0817 - val_loss: 2.9667 - val_acc: 0.0946 - val_mean_absolute_error: 0.0816 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "176/176 [==============================] - 4s 22ms/step - loss: 2.9468 - acc: 0.1095 - mean_absolute_error: 0.0809 - val_loss: 2.9500 - val_acc: 0.0839 - val_mean_absolute_error: 0.0816 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "176/176 [==============================] - 4s 22ms/step - loss: 2.9071 - acc: 0.1130 - mean_absolute_error: 0.0807 - val_loss: 2.8608 - val_acc: 0.1071 - val_mean_absolute_error: 0.0810 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "176/176 [==============================] - 4s 22ms/step - loss: 2.8890 - acc: 0.1130 - mean_absolute_error: 0.0807 - val_loss: 2.8201 - val_acc: 0.1071 - val_mean_absolute_error: 0.0804 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "176/176 [==============================] - 4s 22ms/step - loss: 2.8677 - acc: 0.1170 - mean_absolute_error: 0.0803 - val_loss: 2.8234 - val_acc: 0.1304 - val_mean_absolute_error: 0.0805 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "174/176 [============================>.] - ETA: 0s - loss: 2.8542 - acc: 0.1212 - mean_absolute_error: 0.0803\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "176/176 [==============================] - 4s 22ms/step - loss: 2.8525 - acc: 0.1216 - mean_absolute_error: 0.0803 - val_loss: 2.8404 - val_acc: 0.1125 - val_mean_absolute_error: 0.0810 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "176/176 [==============================] - 4s 22ms/step - loss: 2.8120 - acc: 0.1298 - mean_absolute_error: 0.0801 - val_loss: 2.8025 - val_acc: 0.1464 - val_mean_absolute_error: 0.0804 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "176/176 [==============================] - 4s 22ms/step - loss: 2.7867 - acc: 0.1468 - mean_absolute_error: 0.0797 - val_loss: 2.7890 - val_acc: 0.1571 - val_mean_absolute_error: 0.0803 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "176/176 [==============================] - 4s 22ms/step - loss: 2.7675 - acc: 0.1436 - mean_absolute_error: 0.0795 - val_loss: 2.7846 - val_acc: 0.1518 - val_mean_absolute_error: 0.0801 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "176/176 [==============================] - 4s 22ms/step - loss: 2.7720 - acc: 0.1440 - mean_absolute_error: 0.0796 - val_loss: 2.7866 - val_acc: 0.1429 - val_mean_absolute_error: 0.0800 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "176/176 [==============================] - ETA: 0s - loss: 2.7702 - acc: 0.1458 - mean_absolute_error: 0.0795\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "176/176 [==============================] - 4s 22ms/step - loss: 2.7702 - acc: 0.1458 - mean_absolute_error: 0.0795 - val_loss: 2.7841 - val_acc: 0.1482 - val_mean_absolute_error: 0.0801 - lr: 1.0000e-04\n",
      "Epoch 12: early stopping\n",
      "36/36 [==============================] - 17s 487ms/step - loss: 2.7416 - acc: 0.1583 - mean_absolute_error: 0.0792\n",
      "[2.7416343688964844, 0.15826086699962616, 0.07917343825101852]\n",
      "0.14821428060531616\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor=\"val_loss\", patience=2, min_delta=0.01, factor=0.1, cooldown=4, verbose=1)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01, verbose=1, mode='min', monitor='val_loss')\n",
    "time_callback = TimingCallback()\n",
    "\n",
    "train_generator = ImageDataGenerator().flow_from_directory(diverted_images_train_path, target_size=(224, 224), batch_size=batch_size)\n",
    "valid_generator = ImageDataGenerator().flow_from_directory(diverted_images_valid_path, target_size=(224, 224), batch_size=batch_size)\n",
    "test_generator = ImageDataGenerator().flow_from_directory(diverted_images_test_path, target_size=(224, 224), batch_size=batch_size)\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "def build_model():\n",
    "    input_layer = Input(shape=(224, 224, 3))\n",
    "    x = GlobalAveragePooling2D()(input_layer)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=input_layer, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "#model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=20,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2829 images belonging to 23 classes.\n",
      "Found 575 images belonging to 23 classes.\n",
      "Found 575 images belonging to 23 classes.\n",
      "Epoch 1/5\n",
      "141/141 [==============================] - 4s 28ms/step - loss: 7.1046 - acc: 0.0591 - mean_absolute_error: 0.0827 - val_loss: 3.1373 - val_acc: 0.0482 - val_mean_absolute_error: 0.0831 - lr: 0.0100\n",
      "Epoch 2/5\n",
      "141/141 [==============================] - 4s 27ms/step - loss: 3.1327 - acc: 0.0452 - mean_absolute_error: 0.0830 - val_loss: 3.1350 - val_acc: 0.0446 - val_mean_absolute_error: 0.0831 - lr: 0.0100\n",
      "Epoch 3/5\n",
      "140/141 [============================>.] - ETA: 0s - loss: 3.1351 - acc: 0.0387 - mean_absolute_error: 0.0831\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "141/141 [==============================] - 4s 27ms/step - loss: 3.1351 - acc: 0.0388 - mean_absolute_error: 0.0831 - val_loss: 3.1337 - val_acc: 0.0464 - val_mean_absolute_error: 0.0832 - lr: 0.0100\n",
      "Epoch 4/5\n",
      "141/141 [==============================] - 4s 26ms/step - loss: 3.1351 - acc: 0.0438 - mean_absolute_error: 0.0832 - val_loss: 3.1334 - val_acc: 0.0446 - val_mean_absolute_error: 0.0832 - lr: 1.0000e-03\n",
      "Epoch 4: early stopping\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 3.1359 - acc: 0.0435 - mean_absolute_error: 0.0832\n",
      "[3.135871171951294, 0.043478261679410934, 0.08317580074071884]\n",
      "0.0446428582072258\n"
     ]
    }
   ],
   "source": [
    "#j'augmente le batch size\n",
    "batch_size = 20\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor=\"val_loss\", patience=2, min_delta=0.01, factor=0.1, cooldown=4, verbose=1)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01, verbose=1, mode='min', monitor='val_loss')\n",
    "time_callback = TimingCallback()\n",
    "\n",
    "train_generator = ImageDataGenerator().flow_from_directory(diverted_images_train_path, target_size=(224, 224), batch_size=batch_size)\n",
    "valid_generator = ImageDataGenerator().flow_from_directory(diverted_images_valid_path, target_size=(224, 224), batch_size=batch_size)\n",
    "test_generator = ImageDataGenerator().flow_from_directory(diverted_images_test_path, target_size=(224, 224), batch_size=batch_size)\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "def build_model():\n",
    "    input_layer = Input(shape=(224, 224, 3))\n",
    "    x = GlobalAveragePooling2D()(input_layer)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=input_layer, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "#model.summary()\n",
    "\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2829 images belonging to 23 classes.\n",
      "Found 575 images belonging to 23 classes.\n",
      "Found 575 images belonging to 23 classes.\n",
      "Found 2829 images belonging to 23 classes.\n",
      "Epoch 1/5\n",
      "141/141 [==============================] - 23s 159ms/step - loss: 6.6151 - acc: 0.0541 - mean_absolute_error: 0.0829 - val_loss: 3.1415 - val_acc: 0.0446 - val_mean_absolute_error: 0.0832 - lr: 0.0100\n",
      "Epoch 2/5\n",
      "141/141 [==============================] - 22s 157ms/step - loss: 3.1471 - acc: 0.0395 - mean_absolute_error: 0.0832 - val_loss: 3.1456 - val_acc: 0.0250 - val_mean_absolute_error: 0.0831 - lr: 0.0100\n",
      "Epoch 3/5\n",
      "141/141 [==============================] - ETA: 0s - loss: 3.1487 - acc: 0.0381 - mean_absolute_error: 0.0832\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "141/141 [==============================] - 22s 155ms/step - loss: 3.1487 - acc: 0.0381 - mean_absolute_error: 0.0832 - val_loss: 3.1362 - val_acc: 0.0429 - val_mean_absolute_error: 0.0832 - lr: 0.0100\n",
      "Epoch 4/5\n",
      "141/141 [==============================] - 22s 157ms/step - loss: 3.1365 - acc: 0.0434 - mean_absolute_error: 0.0832 - val_loss: 3.1362 - val_acc: 0.0429 - val_mean_absolute_error: 0.0832 - lr: 1.0000e-03\n",
      "Epoch 4: early stopping\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 3.1360 - acc: 0.0435 - mean_absolute_error: 0.0832\n",
      "[3.1359527111053467, 0.043478261679410934, 0.08317572623491287]\n",
      "0.04285714402794838\n"
     ]
    }
   ],
   "source": [
    "#j'utilise imagedatagenerator\n",
    "batch_size = 20\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor=\"val_loss\", patience=2, min_delta=0.01, factor=0.1, cooldown=4, verbose=1)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01, verbose=1, mode='min', monitor='val_loss')\n",
    "time_callback = TimingCallback()\n",
    "\n",
    "train_generator = ImageDataGenerator().flow_from_directory(diverted_images_train_path, target_size=(224, 224), batch_size=batch_size)\n",
    "valid_generator = ImageDataGenerator().flow_from_directory(diverted_images_valid_path, target_size=(224, 224), batch_size=batch_size)\n",
    "test_generator = ImageDataGenerator().flow_from_directory(diverted_images_test_path, target_size=(224, 224), batch_size=batch_size)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    diverted_images_train_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "def build_model():\n",
    "    input_layer = Input(shape=(224, 224, 3))\n",
    "    x = GlobalAveragePooling2D()(input_layer)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(rate=0.5)(x)  \n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(rate=0.5)(x) \n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=input_layer, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "#model.summary()\n",
    "\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2829 images belonging to 23 classes.\n",
      "Found 575 images belonging to 23 classes.\n",
      "Found 575 images belonging to 23 classes.\n",
      "Epoch 1/5\n",
      "141/141 [==============================] - 23s 157ms/step - loss: 17.4828 - acc: 0.0862 - mean_absolute_error: 0.0806 - val_loss: 17.1917 - val_acc: 0.0929 - val_mean_absolute_error: 0.0803 - lr: 0.0100\n",
      "Epoch 2/5\n",
      "141/141 [==============================] - 22s 156ms/step - loss: 16.5525 - acc: 0.0936 - mean_absolute_error: 0.0800 - val_loss: 15.4780 - val_acc: 0.1304 - val_mean_absolute_error: 0.0786 - lr: 0.0100\n",
      "Epoch 3/5\n",
      "141/141 [==============================] - 22s 156ms/step - loss: 15.7976 - acc: 0.0918 - mean_absolute_error: 0.0800 - val_loss: 14.9341 - val_acc: 0.1089 - val_mean_absolute_error: 0.0796 - lr: 0.0100\n",
      "Epoch 4/5\n",
      "141/141 [==============================] - 22s 159ms/step - loss: 15.0468 - acc: 0.1011 - mean_absolute_error: 0.0794 - val_loss: 14.1103 - val_acc: 0.1393 - val_mean_absolute_error: 0.0796 - lr: 0.0100\n",
      "Epoch 5/5\n",
      "141/141 [==============================] - 22s 156ms/step - loss: 14.3866 - acc: 0.0983 - mean_absolute_error: 0.0799 - val_loss: 13.5341 - val_acc: 0.1321 - val_mean_absolute_error: 0.0788 - lr: 0.0100\n",
      "29/29 [==============================] - 1s 21ms/step - loss: 13.4935 - acc: 0.1357 - mean_absolute_error: 0.0786\n",
      "[13.493517875671387, 0.13565216958522797, 0.07855689525604248]\n",
      "0.13214285671710968\n"
     ]
    }
   ],
   "source": [
    "#modification de l'optimizer avec SGD à la place d'Adam\n",
    "batch_size = 20\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor=\"val_loss\", patience=2, min_delta=0.01, factor=0.1, cooldown=4, verbose=1)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01, verbose=1, mode='min', monitor='val_loss')\n",
    "time_callback = TimingCallback()\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    diverted_images_train_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "valid_generator = ImageDataGenerator().flow_from_directory(diverted_images_valid_path, target_size=(224, 224), batch_size=batch_size)\n",
    "test_generator = ImageDataGenerator().flow_from_directory(diverted_images_test_path, target_size=(224, 224), batch_size=batch_size)\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "def build_model():\n",
    "    input_layer = Input(shape=(224, 224, 3))\n",
    "    x = GlobalAveragePooling2D()(input_layer)\n",
    "    x = Dense(2048, activation='relu', kernel_regularizer=l2(0.01))(x) \n",
    "    x = BatchNormalization()(x)  \n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(1024, activation='relu', kernel_regularizer=l2(0.01))(x)  \n",
    "    x = BatchNormalization()(x) \n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=input_layer, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "#model.summary()\n",
    "\n",
    "optimizer = SGD(learning_rate=0.01) \n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "# Entraînement du modèle\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "141/141 [==============================] - 23s 159ms/step - loss: 3715.6208 - acc: 0.0701 - mean_absolute_error: 0.0809 - val_loss: 503.8104 - val_acc: 0.1107 - val_mean_absolute_error: 0.0773 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 22s 156ms/step - loss: 533.9200 - acc: 0.0865 - mean_absolute_error: 0.0794 - val_loss: 220.6390 - val_acc: 0.1196 - val_mean_absolute_error: 0.0766 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 22s 156ms/step - loss: 327.3020 - acc: 0.0904 - mean_absolute_error: 0.0791 - val_loss: 217.4539 - val_acc: 0.0768 - val_mean_absolute_error: 0.0803 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 22s 155ms/step - loss: 251.5881 - acc: 0.0961 - mean_absolute_error: 0.0786 - val_loss: 146.1829 - val_acc: 0.0804 - val_mean_absolute_error: 0.0800 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - 22s 156ms/step - loss: 200.5815 - acc: 0.0872 - mean_absolute_error: 0.0793 - val_loss: 173.2022 - val_acc: 0.0911 - val_mean_absolute_error: 0.0792 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 22s 158ms/step - loss: 170.7403 - acc: 0.0965 - mean_absolute_error: 0.0786 - val_loss: 127.4542 - val_acc: 0.0750 - val_mean_absolute_error: 0.0805 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "141/141 [==============================] - 22s 158ms/step - loss: 155.6091 - acc: 0.0897 - mean_absolute_error: 0.0791 - val_loss: 115.0756 - val_acc: 0.1018 - val_mean_absolute_error: 0.0781 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "141/141 [==============================] - 22s 156ms/step - loss: 157.3903 - acc: 0.0890 - mean_absolute_error: 0.0792 - val_loss: 124.6599 - val_acc: 0.0821 - val_mean_absolute_error: 0.0797 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "141/141 [==============================] - ETA: 0s - loss: 157.9774 - acc: 0.0847 - mean_absolute_error: 0.0796\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "141/141 [==============================] - 23s 165ms/step - loss: 157.9774 - acc: 0.0847 - mean_absolute_error: 0.0796 - val_loss: 143.3539 - val_acc: 0.1018 - val_mean_absolute_error: 0.0780 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "141/141 [==============================] - 23s 162ms/step - loss: 93.0058 - acc: 0.0972 - mean_absolute_error: 0.0785 - val_loss: 31.9133 - val_acc: 0.1357 - val_mean_absolute_error: 0.0746 - lr: 1.0000e-03\n",
      "29/29 [==============================] - 1s 22ms/step - loss: 29.1805 - acc: 0.1426 - mean_absolute_error: 0.0744\n",
      "[29.18045425415039, 0.14260870218276978, 0.07441241294145584]\n",
      "0.13571429252624512\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    input_layer = Input(shape=(224, 224, 3))\n",
    "    x = GlobalAveragePooling2D()(input_layer)\n",
    "    x = Dense(2048, kernel_initializer='he_normal')(x)  \n",
    "    x = LeakyReLU()(x) \n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(1024, kernel_initializer='he_normal')(x) \n",
    "    x = LeakyReLU()(x)  \n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=input_layer, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "#model.summary()\n",
    "\n",
    "optimizer = Adam(learning_rate=0.01, decay=1e-6)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=10,  \n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2829 images belonging to 23 classes.\n",
      "Found 575 images belonging to 23 classes.\n",
      "Found 575 images belonging to 23 classes.\n",
      "Epoch 1/5\n",
      "141/141 [==============================] - 24s 165ms/step - loss: 477.8160 - acc: 0.0406 - mean_absolute_error: 0.0832 - val_loss: 3.1363 - val_acc: 0.0411 - val_mean_absolute_error: 0.0832 - lr: 0.0100\n",
      "Epoch 2/5\n",
      "141/141 [==============================] - 29s 203ms/step - loss: 3.1394 - acc: 0.0352 - mean_absolute_error: 0.0832 - val_loss: 3.1360 - val_acc: 0.0446 - val_mean_absolute_error: 0.0832 - lr: 0.0100\n",
      "Epoch 3/5\n",
      "141/141 [==============================] - ETA: 0s - loss: 3.1392 - acc: 0.0413 - mean_absolute_error: 0.0832\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "141/141 [==============================] - 27s 188ms/step - loss: 3.1392 - acc: 0.0413 - mean_absolute_error: 0.0832 - val_loss: 3.1358 - val_acc: 0.0429 - val_mean_absolute_error: 0.0832 - lr: 0.0100\n",
      "Epoch 4/5\n",
      "141/141 [==============================] - 26s 182ms/step - loss: 3.1362 - acc: 0.0406 - mean_absolute_error: 0.0832 - val_loss: 3.1355 - val_acc: 0.0429 - val_mean_absolute_error: 0.0832 - lr: 1.0000e-03\n",
      "Epoch 4: early stopping\n",
      "29/29 [==============================] - 1s 27ms/step - loss: 3.1357 - acc: 0.0435 - mean_absolute_error: 0.0832\n",
      "[3.1357290744781494, 0.043478261679410934, 0.08317579329013824]\n",
      "0.04285714402794838\n"
     ]
    }
   ],
   "source": [
    "# modèle CNN\n",
    "batch_size = 20\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor=\"val_loss\", patience=2, min_delta=0.01, factor=0.1, cooldown=4, verbose=1)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01, verbose=1, mode='min', monitor='val_loss')\n",
    "time_callback = TimingCallback()\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    diverted_images_train_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "valid_generator = ImageDataGenerator().flow_from_directory(diverted_images_valid_path, target_size=(224, 224), \n",
    "                                                           batch_size=batch_size)\n",
    "test_generator = ImageDataGenerator().flow_from_directory(diverted_images_test_path, target_size=(224, 224), \n",
    "                                                          batch_size=batch_size)\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "def build_model():\n",
    "    input_layer = Input(shape=(224, 224, 3))\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(input_layer)  \n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x) \n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)  \n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)  \n",
    "    x = Flatten()(x) \n",
    "    x = Dense(128, activation='relu')(x)  \n",
    "    predictions = Dense(num_classes, activation='softmax')(x)  \n",
    "    model = Model(inputs=input_layer, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "#model.summary()\n",
    "\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "141/141 [==============================] - 30s 201ms/step - loss: 3.3234 - acc: 0.1339 - mean_absolute_error: 0.0782 - val_loss: 4.7609 - val_acc: 0.1911 - val_mean_absolute_error: 0.0723 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 26s 186ms/step - loss: 2.6064 - acc: 0.2645 - mean_absolute_error: 0.0710 - val_loss: 2.0890 - val_acc: 0.3536 - val_mean_absolute_error: 0.0633 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 27s 191ms/step - loss: 2.2484 - acc: 0.3489 - mean_absolute_error: 0.0663 - val_loss: 1.8704 - val_acc: 0.4661 - val_mean_absolute_error: 0.0567 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 27s 190ms/step - loss: 2.0356 - acc: 0.3891 - mean_absolute_error: 0.0628 - val_loss: 2.2086 - val_acc: 0.3821 - val_mean_absolute_error: 0.0590 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.8691 - acc: 0.4236 - mean_absolute_error: 0.0603\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "141/141 [==============================] - 26s 182ms/step - loss: 1.8691 - acc: 0.4236 - mean_absolute_error: 0.0603 - val_loss: 1.8850 - val_acc: 0.4554 - val_mean_absolute_error: 0.0545 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 27s 193ms/step - loss: 1.6848 - acc: 0.4870 - mean_absolute_error: 0.0568 - val_loss: 1.2309 - val_acc: 0.6429 - val_mean_absolute_error: 0.0474 - lr: 1.0000e-04\n",
      "Epoch 7/10\n",
      "141/141 [==============================] - 26s 186ms/step - loss: 1.6306 - acc: 0.4991 - mean_absolute_error: 0.0557 - val_loss: 1.2386 - val_acc: 0.6554 - val_mean_absolute_error: 0.0474 - lr: 1.0000e-04\n",
      "Epoch 8/10\n",
      "141/141 [==============================] - 26s 185ms/step - loss: 1.5611 - acc: 0.5301 - mean_absolute_error: 0.0542 - val_loss: 1.2129 - val_acc: 0.6375 - val_mean_absolute_error: 0.0464 - lr: 1.0000e-04\n",
      "Epoch 9/10\n",
      "141/141 [==============================] - 30s 211ms/step - loss: 1.5351 - acc: 0.5294 - mean_absolute_error: 0.0537 - val_loss: 1.1794 - val_acc: 0.6589 - val_mean_absolute_error: 0.0443 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "141/141 [==============================] - 26s 180ms/step - loss: 1.4956 - acc: 0.5479 - mean_absolute_error: 0.0528 - val_loss: 1.1712 - val_acc: 0.6500 - val_mean_absolute_error: 0.0444 - lr: 1.0000e-04\n",
      "29/29 [==============================] - 1s 44ms/step - loss: 1.0584 - acc: 0.6748 - mean_absolute_error: 0.0422\n",
      "[1.0584373474121094, 0.6747826337814331, 0.0422217920422554]\n",
      "0.6499999761581421\n"
     ]
    }
   ],
   "source": [
    "#modèle sequentiel avec Batchnormalization\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=10,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "141/141 [==============================] - 27s 187ms/step - loss: 3.2012 - acc: 0.1367 - mean_absolute_error: 0.0789 - val_loss: 4.0941 - val_acc: 0.1375 - val_mean_absolute_error: 0.0764 - lr: 0.0010\n",
      "Epoch 2/40\n",
      "141/141 [==============================] - 28s 194ms/step - loss: 2.6955 - acc: 0.2342 - mean_absolute_error: 0.0733 - val_loss: 4.0162 - val_acc: 0.2429 - val_mean_absolute_error: 0.0681 - lr: 0.0010\n",
      "Epoch 3/40\n",
      "141/141 [==============================] - 26s 186ms/step - loss: 2.4705 - acc: 0.2876 - mean_absolute_error: 0.0695 - val_loss: 1.8545 - val_acc: 0.4125 - val_mean_absolute_error: 0.0574 - lr: 0.0010\n",
      "Epoch 4/40\n",
      "141/141 [==============================] - 27s 192ms/step - loss: 2.1891 - acc: 0.3510 - mean_absolute_error: 0.0658 - val_loss: 1.6985 - val_acc: 0.4536 - val_mean_absolute_error: 0.0589 - lr: 0.0010\n",
      "Epoch 5/40\n",
      "141/141 [==============================] - 26s 184ms/step - loss: 2.0703 - acc: 0.3902 - mean_absolute_error: 0.0640 - val_loss: 2.0882 - val_acc: 0.4500 - val_mean_absolute_error: 0.0561 - lr: 0.0010\n",
      "Epoch 6/40\n",
      "141/141 [==============================] - 27s 192ms/step - loss: 1.8804 - acc: 0.4311 - mean_absolute_error: 0.0612 - val_loss: 1.5477 - val_acc: 0.5339 - val_mean_absolute_error: 0.0529 - lr: 0.0010\n",
      "Epoch 7/40\n",
      "141/141 [==============================] - 26s 184ms/step - loss: 1.7718 - acc: 0.4496 - mean_absolute_error: 0.0589 - val_loss: 1.3078 - val_acc: 0.6143 - val_mean_absolute_error: 0.0498 - lr: 0.0010\n",
      "Epoch 8/40\n",
      "141/141 [==============================] - 27s 189ms/step - loss: 1.6412 - acc: 0.4941 - mean_absolute_error: 0.0568 - val_loss: 1.2989 - val_acc: 0.6036 - val_mean_absolute_error: 0.0476 - lr: 0.0010\n",
      "Epoch 9/40\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.6019 - acc: 0.5084 - mean_absolute_error: 0.0551\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "141/141 [==============================] - 27s 188ms/step - loss: 1.6019 - acc: 0.5084 - mean_absolute_error: 0.0551 - val_loss: 1.3188 - val_acc: 0.5946 - val_mean_absolute_error: 0.0455 - lr: 0.0010\n",
      "Epoch 10/40\n",
      "141/141 [==============================] - 26s 182ms/step - loss: 1.4705 - acc: 0.5529 - mean_absolute_error: 0.0522 - val_loss: 1.0768 - val_acc: 0.6821 - val_mean_absolute_error: 0.0430 - lr: 1.0000e-04\n",
      "Epoch 11/40\n",
      "141/141 [==============================] - 26s 181ms/step - loss: 1.4107 - acc: 0.5774 - mean_absolute_error: 0.0508 - val_loss: 1.0933 - val_acc: 0.6857 - val_mean_absolute_error: 0.0431 - lr: 1.0000e-04\n",
      "Epoch 12/40\n",
      "141/141 [==============================] - 26s 187ms/step - loss: 1.4045 - acc: 0.5664 - mean_absolute_error: 0.0513 - val_loss: 1.0652 - val_acc: 0.6982 - val_mean_absolute_error: 0.0423 - lr: 1.0000e-04\n",
      "Epoch 13/40\n",
      "141/141 [==============================] - 25s 177ms/step - loss: 1.4050 - acc: 0.5685 - mean_absolute_error: 0.0507 - val_loss: 1.0283 - val_acc: 0.7107 - val_mean_absolute_error: 0.0417 - lr: 1.0000e-04\n",
      "Epoch 14/40\n",
      "141/141 [==============================] - 25s 177ms/step - loss: 1.3309 - acc: 0.5988 - mean_absolute_error: 0.0490 - val_loss: 1.0198 - val_acc: 0.7054 - val_mean_absolute_error: 0.0407 - lr: 1.0000e-04\n",
      "Epoch 15/40\n",
      "141/141 [==============================] - 25s 175ms/step - loss: 1.3675 - acc: 0.5650 - mean_absolute_error: 0.0505 - val_loss: 0.9724 - val_acc: 0.7214 - val_mean_absolute_error: 0.0398 - lr: 1.0000e-04\n",
      "Epoch 16/40\n",
      "141/141 [==============================] - 24s 171ms/step - loss: 1.3523 - acc: 0.5828 - mean_absolute_error: 0.0497 - val_loss: 0.9771 - val_acc: 0.7161 - val_mean_absolute_error: 0.0395 - lr: 1.0000e-04\n",
      "Epoch 17/40\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.3171 - acc: 0.5945 - mean_absolute_error: 0.0489\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "141/141 [==============================] - 24s 171ms/step - loss: 1.3171 - acc: 0.5945 - mean_absolute_error: 0.0489 - val_loss: 0.9942 - val_acc: 0.7036 - val_mean_absolute_error: 0.0389 - lr: 1.0000e-04\n",
      "Epoch 18/40\n",
      "141/141 [==============================] - 25s 175ms/step - loss: 1.2615 - acc: 0.6045 - mean_absolute_error: 0.0475 - val_loss: 0.9454 - val_acc: 0.7232 - val_mean_absolute_error: 0.0381 - lr: 1.0000e-05\n",
      "Epoch 19/40\n",
      "141/141 [==============================] - 25s 174ms/step - loss: 1.2655 - acc: 0.6066 - mean_absolute_error: 0.0478 - val_loss: 0.9409 - val_acc: 0.7321 - val_mean_absolute_error: 0.0382 - lr: 1.0000e-05\n",
      "Epoch 20/40\n",
      "141/141 [==============================] - 25s 178ms/step - loss: 1.3112 - acc: 0.5981 - mean_absolute_error: 0.0480 - val_loss: 0.9306 - val_acc: 0.7250 - val_mean_absolute_error: 0.0378 - lr: 1.0000e-05\n",
      "Epoch 21/40\n",
      "141/141 [==============================] - 25s 174ms/step - loss: 1.2932 - acc: 0.6020 - mean_absolute_error: 0.0481 - val_loss: 0.9353 - val_acc: 0.7304 - val_mean_absolute_error: 0.0376 - lr: 1.0000e-05\n",
      "Epoch 22/40\n",
      "141/141 [==============================] - 25s 175ms/step - loss: 1.2702 - acc: 0.6084 - mean_absolute_error: 0.0475 - val_loss: 0.9166 - val_acc: 0.7304 - val_mean_absolute_error: 0.0376 - lr: 1.0000e-05\n",
      "Epoch 23/40\n",
      "141/141 [==============================] - 25s 174ms/step - loss: 1.2335 - acc: 0.6177 - mean_absolute_error: 0.0471 - val_loss: 0.9334 - val_acc: 0.7250 - val_mean_absolute_error: 0.0378 - lr: 1.0000e-05\n",
      "Epoch 24/40\n",
      "141/141 [==============================] - ETA: 0s - loss: 1.2403 - acc: 0.6209 - mean_absolute_error: 0.0470\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "141/141 [==============================] - 25s 173ms/step - loss: 1.2403 - acc: 0.6209 - mean_absolute_error: 0.0470 - val_loss: 0.9293 - val_acc: 0.7268 - val_mean_absolute_error: 0.0377 - lr: 1.0000e-05\n",
      "Epoch 25/40\n",
      "141/141 [==============================] - 27s 189ms/step - loss: 1.2417 - acc: 0.6063 - mean_absolute_error: 0.0472 - val_loss: 0.9071 - val_acc: 0.7357 - val_mean_absolute_error: 0.0370 - lr: 1.0000e-06\n",
      "Epoch 25: early stopping\n",
      "29/29 [==============================] - 1s 32ms/step - loss: 0.8715 - acc: 0.7391 - mean_absolute_error: 0.0365\n",
      "[0.8714554309844971, 0.739130437374115, 0.03653360903263092]\n",
      "0.7357142567634583\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=40,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meilleur modèle avec augmentation des données\n",
    "batch_size = 16\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(diverted_images_train_path, target_size=(224, 224), \n",
    "                                                    batch_size=batch_size, class_mode='categorical')\n",
    "valid_generator = valid_datagen.flow_from_directory(diverted_images_valid_path, target_size=(224, 224), \n",
    "                                                    batch_size=batch_size, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(diverted_images_test_path, target_size=(224, 224), \n",
    "                                                  batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor=\"val_loss\", patience=2, min_delta=0.01, factor=0.1, cooldown=4, verbose=1)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01, verbose=1, mode='min', monitor='val_loss')\n",
    "time_callback = TimingCallback()\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=20,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meilleur modèle sans augmentation des données\n",
    "batch_size = 16\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(diverted_images_train_path, target_size=(224, 224), \n",
    "                                                    batch_size=batch_size, class_mode='categorical')\n",
    "valid_generator = valid_datagen.flow_from_directory(diverted_images_valid_path, target_size=(224, 224), \n",
    "                                                    batch_size=batch_size, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(diverted_images_test_path, target_size=(224, 224), \n",
    "                                                  batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor=\"val_loss\", patience=2, min_delta=0.01, factor=0.1, cooldown=4, verbose=1)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01, verbose=1, mode='min', monitor='val_loss')\n",
    "time_callback = TimingCallback()\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=20,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#augmentation de la complexité du modèle, avec augmentation de données\n",
    "batch_size = 16\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(diverted_images_train_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "valid_generator = valid_datagen.flow_from_directory(diverted_images_valid_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(diverted_images_test_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor=\"val_loss\", patience=2, min_delta=0.01, factor=0.1, cooldown=4, verbose=1)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01, verbose=1, mode='min', monitor='val_loss')\n",
    "time_callback = TimingCallback()\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=10,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "\n",
    "#j'arrête l'execution, le modèle ne performe pas du tout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2829 images belonging to 23 classes.\n",
      "Found 575 images belonging to 23 classes.\n",
      "Found 575 images belonging to 23 classes.\n",
      "Epoch 1/10\n",
      "  6/176 [>.............................] - ETA: 59s - loss: 4.3636 - acc: 0.0729 - mean_absolute_error: 0.0815 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1116s vs `on_train_batch_end` time: 0.2289s). Check your callbacks.\n",
      "176/176 [==============================] - 58s 319ms/step - loss: 2.7657 - acc: 0.2609 - mean_absolute_error: 0.0705 - val_loss: 5.4471 - val_acc: 0.0536 - val_mean_absolute_error: 0.0827 - lr: 1.0000e-04\n",
      "Epoch 2/10\n",
      "176/176 [==============================] - 54s 306ms/step - loss: 2.2880 - acc: 0.3502 - mean_absolute_error: 0.0642 - val_loss: 6.1232 - val_acc: 0.0821 - val_mean_absolute_error: 0.0814 - lr: 1.0000e-04\n",
      "Epoch 3/10\n",
      "176/176 [==============================] - 53s 300ms/step - loss: 2.0940 - acc: 0.3903 - mean_absolute_error: 0.0615 - val_loss: 3.1030 - val_acc: 0.2071 - val_mean_absolute_error: 0.0714 - lr: 1.0000e-04\n",
      "Epoch 4/10\n",
      "176/176 [==============================] - ETA: 0s - loss: 1.9443 - acc: 0.4390 - mean_absolute_error: 0.0582"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 73\u001b[0m\n\u001b[0;32m     70\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m     71\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_absolute_error\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 73\u001b[0m history_diverted \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mtrain_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mvalid_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mreduce_learning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Évaluation du modèle\u001b[39;00m\n\u001b[0;32m     81\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_generator)\n",
      "File \u001b[1;32md:\\Code\\Datascience\\reco_oiseau_jan24bds\\venvGPU\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Code\\Datascience\\reco_oiseau_jan24bds\\venvGPU\\lib\\site-packages\\keras\\engine\\training.py:1445\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1432\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[0;32m   1433\u001b[0m       x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m   1434\u001b[0m       y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1443\u001b[0m       model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1444\u001b[0m       steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution)\n\u001b[1;32m-> 1445\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1447\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1450\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1454\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1457\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m   1458\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32md:\\Code\\Datascience\\reco_oiseau_jan24bds\\venvGPU\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Code\\Datascience\\reco_oiseau_jan24bds\\venvGPU\\lib\\site-packages\\keras\\engine\\training.py:1756\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1755\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 1756\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1757\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1758\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32md:\\Code\\Datascience\\reco_oiseau_jan24bds\\venvGPU\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Code\\Datascience\\reco_oiseau_jan24bds\\venvGPU\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\Code\\Datascience\\reco_oiseau_jan24bds\\venvGPU\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Code\\Datascience\\reco_oiseau_jan24bds\\venvGPU\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Code\\Datascience\\reco_oiseau_jan24bds\\venvGPU\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\Code\\Datascience\\reco_oiseau_jan24bds\\venvGPU\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32md:\\Code\\Datascience\\reco_oiseau_jan24bds\\venvGPU\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(diverted_images_train_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "valid_generator = valid_datagen.flow_from_directory(diverted_images_valid_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(diverted_images_test_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor=\"val_loss\", patience=2, min_delta=0.01, factor=0.1, cooldown=4, verbose=1)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01, verbose=1, mode='min', monitor='val_loss')\n",
    "time_callback = TimingCallback()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), padding='same', input_shape=(224, 224, 3)))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=10,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "# Évaluation du modèle\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(diverted_images_train_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "valid_generator = valid_datagen.flow_from_directory(diverted_images_valid_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(diverted_images_test_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor=\"val_loss\", patience=2, min_delta=0.01, factor=0.1, cooldown=4, verbose=1)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01, verbose=1, mode='min', monitor='val_loss')\n",
    "time_callback = TimingCallback()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), padding='same', input_shape=(224, 224, 3)))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(1024, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(1024, (3, 3), padding='same'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=10,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])\n",
    "#j'arrête le modèle ne semble pas du tout performant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modèle sequentiel avec Batchnormalization\n",
    "\n",
    "batch_size = 16\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(diverted_images_train_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "valid_generator = valid_datagen.flow_from_directory(diverted_images_valid_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(diverted_images_test_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor=\"val_loss\", patience=2, min_delta=0.01, factor=0.1, cooldown=4, verbose=1)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01, verbose=1, mode='min', monitor='val_loss')\n",
    "time_callback = TimingCallback()\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=10,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(diverted_images_train_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "valid_generator = valid_datagen.flow_from_directory(diverted_images_valid_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(diverted_images_test_path, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor=\"val_loss\", patience=2, min_delta=0.01, factor=0.1, cooldown=4, verbose=1)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01, verbose=1, mode='min', monitor='val_loss')\n",
    "time_callback = TimingCallback()\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "history_diverted = model.fit(train_generator,\n",
    "                    epochs=10,\n",
    "                    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=valid_generator.samples//valid_generator.batch_size,\n",
    "                    callbacks=[reduce_learning_rate, early_stopping, time_callback], verbose=1)\n",
    "\n",
    "\n",
    "test_accuracy = model.evaluate(test_generator)\n",
    "print(test_accuracy)\n",
    "print(history_diverted.history['val_acc'][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
