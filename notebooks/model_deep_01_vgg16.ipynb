{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Code\\Datascience\\BirdRecognation\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time, cv2\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "import const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création de datagenerator pour modifier nos images au fil des epochs\n",
    "train_data_generator = ImageDataGenerator(preprocessing_function = preprocess_input,\n",
    "                                          rescale=1.0 / 255, #Normalisation\n",
    "                                          featurewise_center=True, #Soustraction de la valeur moyenne de chaque pixel\n",
    "                                          featurewise_std_normalization=True,\n",
    "                                          rotation_range = 30,\n",
    "                                          horizontal_flip = True)\n",
    "valid_data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 46958 images belonging to 443 classes.\n",
      "Found 9746 images belonging to 443 classes.\n",
      "Found 9746 images belonging to 443 classes.\n"
     ]
    }
   ],
   "source": [
    "#Creation des iterators sur les images depuis les dossiers\n",
    "dataset_used = const.DATASET_CLEAN_WO_BACKGROUND\n",
    "train_path = os.path.join(dataset_used, \"train\")\n",
    "valid_path = os.path.join(dataset_used, \"valid\")\n",
    "test_path = os.path.join(dataset_used, \"test\")\n",
    "batch_size = 16\n",
    "train_generator = train_data_generator.flow_from_directory(directory=train_path,\n",
    "                                                           class_mode = \"sparse\",\n",
    "                                                           target_size=(224,224),\n",
    "                                                           batch_size=batch_size)\n",
    "valid_generator = valid_data_generator.flow_from_directory(directory=valid_path,\n",
    "                                                           class_mode = \"sparse\",\n",
    "                                                           target_size=(224,224),\n",
    "                                                           batch_size=batch_size)\n",
    "test_generator = test_data_generator.flow_from_directory(directory=test_path,\n",
    "                                                           class_mode = \"sparse\",\n",
    "                                                           target_size=(224,224),\n",
    "                                                           batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_VGG16(n_class, nb_defroze):\n",
    "    #Creation du model\n",
    "    model = Sequential()\n",
    "    base_model = VGG16(weights='imagenet', include_top=False) \n",
    "    n_class = 443\n",
    "    # Freezer les couches du VGG16\n",
    "    for layer in base_model.layers[:len(base_model.layers) - nb_defroze]: \n",
    "        layer.trainable = False\n",
    "\n",
    "    model.add(base_model) # Ajout du modèle VGG16\n",
    "    model.add(GlobalAveragePooling2D()) \n",
    "    model.add(Dense(1024,activation='relu'))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(n_class, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc', 'mean_absolute_error'])\n",
    "    return model\n",
    "\n",
    "def run_model(model, train_generator, valid_generator, model_name):\n",
    "    nb_image_train = train_generator.samples\n",
    "    nb_image_valid = valid_generator.samples\n",
    "    history = model.fit(train_generator,\n",
    "                              epochs=8, \n",
    "                              steps_per_epoch=nb_image_train//batch_size,\n",
    "                              validation_data=valid_generator, \n",
    "                              validation_steps=nb_image_valid//batch_size)\n",
    "    model_json = model.to_json()\n",
    "    with open(model_name + \".json\", \"w\") as file:\n",
    "        file.write(model_json)\n",
    "\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_name + \".h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "    test_accuracy = model.evaluate(test_generator)\n",
    "    print(f\"Test accuracy: {test_accuracy:.2f}\")\n",
    "    print(test_accuracy)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_model_wo_defroze = create_model_VGG16(n_class=443, nb_defroze=0)\n",
    "history_model_wo_defroze = run_model(model_model_wo_defroze, train_generator, valid_generator, \"model_wo_defroze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_model_4_defroze = create_model_VGG16(n_class=443, nb_defroze=-4)\n",
    "history_model_4_defroze = run_model(model_model_4_defroze, train_generator, valid_generator, \"model_4_defroze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_model_4_defroze.evaluate(test_generator)\n",
    "print(\"%s: %.2f%%\" % (model_model_4_defroze.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.38      0.37        21\n",
      "           1       0.00      0.00      0.00        21\n",
      "           2       0.14      0.14      0.14        21\n",
      "           3       0.24      0.33      0.28        21\n",
      "           4       0.23      0.33      0.27        21\n",
      "\n",
      "    accuracy                           0.24       105\n",
      "   macro avg       0.19      0.24      0.21       105\n",
      "weighted avg       0.19      0.24      0.21       105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pred_labels = model.predict(test_generator)\n",
    "true_labels = test_generator.classes\n",
    "predicted_classes = np.argmax(pred_labels, axis=1)\n",
    "\n",
    "print(classification_report(true_labels, predicted_classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model_model_4_defroze.to_json()\n",
    "with open(\"model_model_4_defroze.json\", \"w\") as file:\n",
    "    file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model_model_4_defroze.save_weights(\"model_model_4_defroze.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "with open('model_model_4_defroze.json', 'r') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "model_model_4_defroze = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_model_4_defroze.load_weights(\"model_model_4_defroze.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "model_model_4_defroze.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
